{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9406691,"sourceType":"datasetVersion","datasetId":5711303},{"sourceId":9407330,"sourceType":"datasetVersion","datasetId":5711844}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-15T23:17:55.533848Z","iopub.execute_input":"2024-09-15T23:17:55.534252Z","iopub.status.idle":"2024-09-15T23:17:55.543436Z","shell.execute_reply.started":"2024-09-15T23:17:55.534202Z","shell.execute_reply":"2024-09-15T23:17:55.542429Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"/kaggle/input/sample-train-ml-brain/train_800.csv\n/kaggle/input/sample-train-ml-brain/output_1000.csv\n/kaggle/input/sample-train-ml-brain/test_200.csv\n/kaggle/input/balanced-ml-amazon-tester/B_train_800.csv\n/kaggle/input/balanced-ml-amazon-tester/B_test_200.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluate_model(model, data_loader, criterion, device):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    all_predictions = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for embedding, z, target in data_loader:\n            embedding, z, target = embedding.to(device), z.to(device), target.to(device)\n            output = model(embedding, z)\n            test_loss += criterion(output, target).item()\n            _, predicted = output.max(1)\n            total += target.size(0)\n            correct += predicted.eq(target.max(1)[1]).sum().item()\n            \n            all_predictions.extend(predicted.cpu().numpy())\n            all_targets.extend(target.max(1)[1].cpu().numpy())\n    \n    avg_test_loss = test_loss / len(data_loader)\n    accuracy = correct / total\n    f1 = f1_score(all_targets, all_predictions, average='weighted')\n    \n    return avg_test_loss, accuracy, f1\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T23:17:55.545354Z","iopub.execute_input":"2024-09-15T23:17:55.545670Z","iopub.status.idle":"2024-09-15T23:17:55.556533Z","shell.execute_reply.started":"2024-09-15T23:17:55.545638Z","shell.execute_reply":"2024-09-15T23:17:55.555476Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nimport ast\nfrom torch.optim.lr_scheduler import StepLR\nfrom sklearn.metrics import f1_score\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\nclass BRAIN(nn.Module):\n    def __init__(self, input_size=1536, hidden_size=1536, z_size=4, output_size=4):\n        super(BRAIN, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size * z_size, 6144)\n        self.dropout1 = nn.Dropout(0.4)\n        self.fc4 = nn.Linear(6144, 1024)\n        self.dropout2 = nn.Dropout(0.2)\n        self.fc3 = nn.Linear(1024, output_size)\n\n    def forward(self, embedding, z):\n        x = self.fc1(embedding)\n        x = torch.matmul(x.unsqueeze(2), z.unsqueeze(1))\n        x = x.view(x.size(0), -1)\n        x = self.fc2(x)\n        x = self.dropout1(x)\n        x = F.relu(x)\n        x = self.fc4(x)\n        x = self.dropout2(x)\n        x = F.relu(x)\n        x = self.fc3(x)\n        return x\n\nclass CustomDataset(Dataset):\n    def __init__(self, df):\n        self.data = df\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        embedding = np.array(ast.literal_eval(self.data.iloc[idx]['EMBEDDING']), dtype=np.float32)\n        z_list = np.array(ast.literal_eval(self.data.iloc[idx]['z_bar']), dtype=np.float32)\n        output = np.array(self.data.iloc[idx]['output'], dtype=np.float32)\n        \n        return torch.tensor(embedding), torch.tensor(z_list), torch.tensor(output)\n\ndef train(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device):\n    for epoch in tqdm(range(num_epochs)):\n        model.train()\n        train_loss = 0\n        for batch_idx, (embedding, z, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            embedding, z, target = embedding.to(device), z.to(device), target.to(device)\n            output = model(embedding, z)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        \n        avg_train_loss = train_loss / len(train_loader)\n        \n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for embedding, z, target in val_loader:\n                embedding, z, target = embedding.to(device), z.to(device), target.to(device)\n                output = model(embedding, z)\n                loss = criterion(output, target)\n                val_loss += loss.item()\n        \n        avg_val_loss = val_loss / len(val_loader)\n        print(f'Epoch {epoch}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n        \n        scheduler.step()\n\ndef save_model(model, path):\n    torch.save(model.state_dict(), path)\n    print(f\"Model saved to {path}\")\n\ndef load_model(model, path):\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    print(f\"Model loaded from {path}\")\n\n# Example usage\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_size = 1536\nhidden_size = 1536\nz_size = 4\noutput_size = 4\n\nmodel = BRAIN(input_size, hidden_size, z_size, output_size)\nmodel = model.to(device)\n\n# Set up optimizer with initial learning rate\ninitial_lr = 1e-3\noptimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n\n# Set up learning rate scheduler\nscheduler = StepLR(optimizer, step_size=2, gamma=0.1)  # Decrease LR by a factor of 10 every 5 epochs\n\ncriterion = nn.CrossEntropyLoss()\n\n# Create datasets and data loaders\nds=load_dataset(\"Hemabhushan/AmazonMLChallengeStage1\",\"balanced\",num_proc=3)\n\ndf=ds[\"train\"].to_pandas()\n\n# df = df[:100]\ntrain_dataset,test_dataset = train_test_split(df, test_size=0.2, random_state=42)\n\ntrain_dataset = CustomDataset(train_dataset)\ntest_dataset = CustomDataset(test_dataset)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Train the model\ntrain(model, optimizer, scheduler, criterion, train_loader, test_loader, num_epochs=15, device=device)\n\n# Save the model after training\nsave_model(model, 'BRAIN_v1.pth')\n\n\ntest_loss, accuracy, f1 = evaluate_model(model, test_loader, criterion, device)\n\nprint(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T23:17:55.557912Z","iopub.execute_input":"2024-09-15T23:17:55.558449Z","iopub.status.idle":"2024-09-15T23:18:02.898139Z","shell.execute_reply.started":"2024-09-15T23:17:55.558404Z","shell.execute_reply":"2024-09-15T23:18:02.897099Z"},"trusted":true},"execution_count":139,"outputs":[{"name":"stderr","text":" 10%|█         | 1/10 [00:00<00:03,  2.26it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 0, Train Loss: 1.233812, Val Loss: 3.603037, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 2/10 [00:00<00:03,  2.27it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Train Loss: 2.573632, Val Loss: 1.026445, LR: 0.001000\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 3/10 [00:01<00:03,  2.27it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Train Loss: 1.237387, Val Loss: 1.037522, LR: 0.000100\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 4/10 [00:01<00:02,  2.27it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Train Loss: 1.102089, Val Loss: 1.034035, LR: 0.000100\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 5/10 [00:02<00:02,  2.27it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Train Loss: 0.981924, Val Loss: 1.026312, LR: 0.000010\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 6/10 [00:02<00:01,  2.27it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Train Loss: 1.024327, Val Loss: 1.015721, LR: 0.000010\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 7/10 [00:03<00:01,  2.26it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Train Loss: 0.997846, Val Loss: 1.014477, LR: 0.000001\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 8/10 [00:03<00:00,  2.26it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Train Loss: 1.008534, Val Loss: 1.013138, LR: 0.000001\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 9/10 [00:03<00:00,  2.26it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Train Loss: 1.017653, Val Loss: 1.012975, LR: 0.000000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:04<00:00,  2.26it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Train Loss: 1.028650, Val Loss: 1.012787, LR: 0.000000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Model saved to BRAIN_v1.pth\nTest Loss: 1.0128, Accuracy: 0.6000, F1 Score: 0.5746\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import HfApi, HfFolder\nfrom huggingface_hub import login\nos.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\nlogin(token=\"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\")\n\ntry:\n    # Replace these with your actual values\n    # The name you want for your model repo on Hugging Face\n    model_name = 'Hemabhushan/amazonmlchallenge2024_model'\n    file_path = 'BRAIN_v1.pth'  # Path to your .pth file\n\n    # Authenticate using your token (assumes you've already logged in with 'login()')\n    api = HfApi()\n\n    # Upload the .pth file\n    api.upload_file(\n        path_or_fileobj=file_path,\n        path_in_repo='pytorch_model.bin',  # This will be the file name in the repo\n        repo_id=model_name,\n        token=\"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\"\n    )\n\n    print(\n        f\"Uploaded {file_path} to the Hugging Face model repository '{model_name}'\")\nexcept Exception as e:\n    print(f'Model Uploading failed with error :{e}')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T23:18:02.900015Z","iopub.execute_input":"2024-09-15T23:18:02.900373Z","iopub.status.idle":"2024-09-15T23:18:02.905399Z","shell.execute_reply.started":"2024-09-15T23:18:02.900337Z","shell.execute_reply":"2024-09-15T23:18:02.904400Z"},"trusted":true},"execution_count":140,"outputs":[]}]}