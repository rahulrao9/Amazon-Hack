{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-15T14:21:57.066994Z","iopub.status.busy":"2024-09-15T14:21:57.066104Z","iopub.status.idle":"2024-09-15T14:21:58.759874Z","shell.execute_reply":"2024-09-15T14:21:58.759177Z","shell.execute_reply.started":"2024-09-15T14:21:57.066945Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T14:21:58.761435Z","iopub.status.busy":"2024-09-15T14:21:58.761094Z","iopub.status.idle":"2024-09-15T14:22:06.799633Z","shell.execute_reply":"2024-09-15T14:22:06.798801Z","shell.execute_reply.started":"2024-09-15T14:21:58.761408Z"},"trusted":true},"outputs":[],"source":["!rm /opt/conda/lib/libcurl.so.4 \n","!ln -s /usr/lib/x86_64-linux-gnu/libcurl.so.4.8.0 /opt/conda/lib/libcurl.so.4\n","!add-apt-repository ppa:alex-p/tesseract-ocr5 -y\n","!apt update\n","!apt install -y tesseract-ocr\n","!apt install tesseract-ocr-eng\n","!tesseract --version"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T14:22:06.801102Z","iopub.status.busy":"2024-09-15T14:22:06.800839Z","iopub.status.idle":"2024-09-15T14:22:10.942308Z","shell.execute_reply":"2024-09-15T14:22:10.941435Z","shell.execute_reply.started":"2024-09-15T14:22:06.801073Z"},"trusted":true},"outputs":[],"source":["!pip install pytesseract"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T14:22:10.945001Z","iopub.status.busy":"2024-09-15T14:22:10.944703Z","iopub.status.idle":"2024-09-15T14:22:18.381374Z","shell.execute_reply":"2024-09-15T14:22:18.380524Z","shell.execute_reply.started":"2024-09-15T14:22:10.944971Z"},"trusted":true},"outputs":[],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T14:22:18.382807Z","iopub.status.busy":"2024-09-15T14:22:18.382545Z","iopub.status.idle":"2024-09-15T14:22:43.921925Z","shell.execute_reply":"2024-09-15T14:22:43.920927Z","shell.execute_reply.started":"2024-09-15T14:22:18.382778Z"},"trusted":true},"outputs":[],"source":["import re\n","import difflib\n","import pytesseract\n","from pytesseract import Output\n","import cv2\n","from typing import List, Tuple, Dict\n","import matplotlib.pyplot as plt\n","import difflib\n","from typing import List, Dict\n","from collections import Counter\n","import numpy as np\n","import pandas as pd\n","import pytesseract\n","from pytesseract import Output\n","from PIL import Image\n","import cv2\n","import re\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from typing import List, Tuple, Dict\n","import math\n","import torch.nn.functional as F\n","import warnings\n","import os\n","import requests\n","from io import BytesIO\n","from datasets import Dataset\n","import multiprocessing\n","import threading\n","import random\n","import time\n","warnings.filterwarnings(\"ignore\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T14:22:43.923551Z","iopub.status.busy":"2024-09-15T14:22:43.923112Z","iopub.status.idle":"2024-09-15T14:22:43.929305Z","shell.execute_reply":"2024-09-15T14:22:43.928551Z","shell.execute_reply.started":"2024-09-15T14:22:43.923522Z"},"trusted":true},"outputs":[],"source":["def random_operations_on_tpu():\n","    import torch\n","    import torch_xla.core.xla_model as xm\n","    import numpy as np\n","\n","    # Create random tensors\n","    tensor_a = torch.tensor(np.random.rand(4, 4), dtype=torch.float32)\n","    tensor_b = torch.tensor(np.random.rand(4, 4), dtype=torch.float32)\n","\n","    # Move tensors to the TPU\n","    device = xm.xla_device()\n","    tensor_a = tensor_a.to(device)\n","    tensor_b = tensor_b.to(device)\n","\n","    # Randomly choose an operation: addition or multiplication\n","    operation = random.choice(['add', 'multiply'])\n","\n","    if operation == 'add':\n","        result = tensor_a + tensor_b\n","    else:\n","        result = tensor_a * tensor_b\n","\n","    # Move the result back to the CPU\n","    result = result.cpu()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T14:22:43.930748Z","iopub.status.busy":"2024-09-15T14:22:43.930486Z","iopub.status.idle":"2024-09-15T14:22:43.942871Z","shell.execute_reply":"2024-09-15T14:22:43.942144Z","shell.execute_reply.started":"2024-09-15T14:22:43.930720Z"},"trusted":true},"outputs":[],"source":["def run_in_thread():\n","    while True:\n","        # Create a new thread for the operation\n","        t = threading.Thread(target=random_operations_on_tpu)\n","        t.start()\n","\n","        # Wait for the thread to finish before starting a new one\n","        t.join()\n","\n","        # Sleep for a random time between 2 to 3 minutes\n","        sleep_time = random.randint(120, 180)  # 120 seconds to 180 seconds\n","        time.sleep(sleep_time)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T14:22:43.944498Z","iopub.status.busy":"2024-09-15T14:22:43.944236Z","iopub.status.idle":"2024-09-15T14:22:43.980734Z","shell.execute_reply":"2024-09-15T14:22:43.979921Z","shell.execute_reply.started":"2024-09-15T14:22:43.944473Z"},"trusted":true},"outputs":[],"source":["class BoundingBox:\n","    def __init__(self, x1: float, y1: float, x2: float, y2: float, text: str):\n","        self.x1 = x1\n","        self.y1 = y1\n","        self.x2 = x2\n","        self.y2 = y2\n","        self.text = text\n","        self.value = None\n","        self.unit = None\n","        self.infer_text()\n","        print(self.value)\n","    \n","    def get_centroid(self) -> Tuple[float, float]:\n","        return ((self.x1 + self.x2) / 2, (self.y1 + self.y2) / 2)\n","    \n","    def infer_text(self):\n","        value, unit = self.text.split()\n","        self.value = float(value)\n","        self.unit = str(unit)\n","\n","# Abbreviation map for units\n","abbreviation_map = {\n","    'in': 'inch',\n","    'cm': 'centimetre',\n","    'ft': 'foot',\n","    'm': 'metre',\n","    'mm': 'millimetre',\n","    'yd': 'yard',\n","    'g': 'gram',\n","    'kg': 'kilogram',\n","    'mg': 'milligram',\n","    'ug': 'microgram',\n","    'oz': 'ounce',\n","    'lb': 'pound',\n","    't': 'ton',\n","    'v': 'volt',\n","    'mv': 'millivolt',\n","    'kv': 'kilovolt',\n","    'w': 'watt',\n","    'kw': 'kilowatt',\n","    'ml': 'millilitre',\n","    'l': 'litre',\n","    'dl': 'decilitre',\n","    'cl': 'centilitre',\n","    'gal': 'gallon',\n","    'pt': 'pint',\n","    'qt': 'quart',\n","    'cu in': 'cubic inch',\n","    'cu ft': 'cubic foot',\n","}\n","\n","entity_unit_map = {\n","    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n","    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n","    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n","    'item_weight': {'gram',\n","        'kilogram',\n","        'microgram',\n","        'milligram',\n","        'ounce',\n","        'pound',\n","        'ton'},\n","    'maximum_weight_recommendation': {'gram',\n","        'kilogram',\n","        'microgram',\n","        'milligram',\n","        'ounce',\n","        'pound',\n","        'ton'},\n","    'voltage': {'kilovolt', 'millivolt', 'volt'},\n","    'wattage': {'kilowatt', 'watt'},\n","    'item_volume': {'centilitre',\n","        'cubic foot',\n","        'cubic inch',\n","        'cup',\n","        'decilitre',\n","        'fluid ounce',\n","        'gallon',\n","        'imperial gallon',\n","        'litre',\n","        'microlitre',\n","        'millilitre',\n","        'pint',\n","        'quart'}\n","}\n","\n","# Combine both abbreviations and full names into a single set for matching\n","valid_units = set(abbreviation_map.keys()).union(set(abbreviation_map.values()))\n","\n","def correct_ocr_errors(text) -> str:\n","    # print(\"Input text:\", text)\n","    # Common OCR errors and their corrections for specified units\n","    corrections = {\n","        # Centimetre\n","        \"centimetre\": \"centimetre\", \"centimeter\": \"centimetre\",\n","        \"centimetres\": \"centimetre\", \"centimeters\": \"centimetre\",\n","        \"centlmetre\": \"centimetre\", \"centlmetres\": \"centimetre\",\n","        \"cm\": \"centimetre\", \"CM\": \"centimetre\",\n","        # Foot\n","        \"ft\": \"foot\", \"feet\": \"foot\", \"foots\": \"foot\",\n","        # Millimetre\n","        \"millimetre\": \"millimetre\", \"millimeter\": \"millimetre\",\n","        \"millimetres\": \"millimetre\", \"millimeters\": \"millimetre\",\n","        \"milimetre\": \"millimetre\", \"milimetres\": \"millimetre\",\n","        \"mm\": \"millimetre\",\n","        # Metre\n","        \"metre\": \"metre\", \"meter\": \"metre\",\n","        \"metres\": \"metre\", \"meters\": \"metre\",\n","        \"m\": \"metre\",\n","        # Inch\n","        \"inch\": \"inch\", \"inches\": \"inch\",\n","        \"inche\": \"inch\", \"inchs\": \"inch\",\n","        \"inoh\": \"inch\", \"lnch\": \"inch\",\n","        \"\\\"\": \"inch\", 'Inchy': \"inch\", \n","        \"inchy\": \"inch\",\n","        # Yard\n","        \"yard\": \"yard\", \"yards\": \"yard\",\n","        \"yrd\": \"yard\", \"yrds\": \"yard\",\n","        \"yd\": \"yard\", \"yds\": \"yard\",\n","    }\n","\n","    # Function to correct numbers\n","    def correct_number(match):\n","        num = match.group(0)\n","        corrected = num.replace('O', '0').replace('l', '1').replace('I', '1')\n","        return corrected\n","\n","    # Correct numbers (replace 'O' with '0', 'l' or 'I' with '1')\n","    text = re.sub(r'\\d+', correct_number, text)\n","\n","    # Correct spacing issues (e.g., \"34. 5\" to \"34.5\")\n","    text = re.sub(r'(\\d+)\\s*\\.\\s*(\\d+)', r'\\1.\\2', text)\n","\n","    # Split the text into words\n","    words = text.split()\n","\n","    # Process each word\n","    for i, word in enumerate(words):\n","        # Check if the word has a number followed by a unit\n","        match = re.match(r'(\\d+)([a-zA-Z]+)', word)\n","        if match:\n","            number, unit = match.groups()\n","            lower_unit = unit.lower()\n","            \n","            # Check if the unit is in our corrections dictionary\n","            if lower_unit in corrections:\n","                corrected_unit = corrections[lower_unit]\n","                words[i] = f\"{number} {corrected_unit}\"\n","            else:\n","                # Use difflib to find the closest match for the unit\n","                close_matches = difflib.get_close_matches(lower_unit, corrections.keys(), n=1, cutoff=0.8)\n","                if close_matches:\n","                    corrected_unit = corrections[close_matches[0]]\n","                    words[i] = f\"{number} {corrected_unit}\"\n","        else:\n","            # Process standalone words without numbers\n","            lower_word = word.lower()\n","            if lower_word in corrections:\n","                words[i] = corrections[lower_word]\n","            else:\n","                close_matches = difflib.get_close_matches(lower_word, corrections.keys(), n=1, cutoff=0.8)\n","                if close_matches:\n","                    words[i] = corrections[close_matches[0]]\n","\n","    # Join the words back into a string\n","    corrected_text = ' '.join(words)\n","    # print(\"Output text:\", corrected_text)\n","    return corrected_text\n","\n","def extract_value_from_ocr(ocr_text):\n","    if pd.isna(ocr_text) or not isinstance(ocr_text, str):\n","        return None  # Return None if OCR text is invalid\n","\n","    # Pattern to capture floats/integers followed by units\n","    pattern = r'(\\d+\\.?\\d*)\\s*([a-zA-Z\\s]+)'  \n","    matches = re.findall(pattern, ocr_text)\n","\n","    # print(matches)\n","\n","    extracted_value = None\n","    for value, unit in matches:\n","        value = str(float(value))\n","        unit = unit.strip().lower()  # Normalize the unit to lowercase\n","        \n","        # If the unit is an abbreviation, replace it with the full unit\n","        if unit in abbreviation_map:\n","            full_unit = abbreviation_map[unit]\n","        else:\n","            # If the unit is already a full unit, use it directly\n","            full_unit = unit if unit in abbreviation_map.values() else correct_ocr_errors(value + ' ' + unit)\n","\n","        if full_unit:\n","            extracted_value = value + ' ' + full_unit\n","            break\n","    \n","    if not extracted_value or len(extracted_value.split()) != 2:\n","        return None\n","\n","    # print(\"Extracted value:\", extracted_value, ocr_text)\n","    # return extracted_value\n","    final_value, final_unit = extracted_value.split()\n","    if final_unit in entity_unit_map['width'] or final_unit in entity_unit_map['depth'] or final_unit in entity_unit_map['height']:\n","        return extracted_value\n","\n","    return None\n","\n","def extract_bboxes(img):\n","    # Load the image\n","    # img = cv2.imread(image_path)\n","    \n","    # Use pytesseract to get bounding boxes\n","    boxes = pytesseract.image_to_data(img, output_type=Output.DICT, config='--psm 12', lang='eng')\n","    \n","    # Extract bounding boxes coordinates and text\n","    n_boxes = len(boxes['text'])\n","    bboxes = []\n","    img = np.array(img)\n","    img_height, img_width = img.shape[:2]  # Get image dimensions for boundary checks\n","    \n","    for i in range(n_boxes):\n","        if boxes['text'][i].strip():\n","            # Original bounding box coordinates\n","            x, y, w, h = boxes['left'][i], boxes['top'][i], boxes['width'][i], boxes['height'][i]\n","            \n","            # Expand the bounding box by 25% in all directions\n","            x_expansion = int(0.25 * w)\n","            y_expansion = int(0.25 * h)\n","            \n","            # Calculate new coordinates\n","            x_new = max(0, x - x_expansion)  # Ensure not going out of image bounds\n","            y_new = max(0, y - y_expansion)\n","            w_new = min(img_width, x + w + x_expansion)  # Ensure not exceeding image width\n","            h_new = min(img_height, y + h + y_expansion)  # Ensure not exceeding image height\n","            \n","            bboxes.append([x_new, y_new, w_new, h_new])\n","    \n","    return img, bboxes\n","\n","def merge_bboxes(bboxes, max_bboxes=3):\n","    def compute_iou(box1, box2):\n","        # Compute Intersection Over Union (IoU) between two boxes\n","        x1 = max(box1[0], box2[0])\n","        y1 = max(box1[1], box2[1])\n","        x2 = min(box1[2], box2[2])\n","        y2 = min(box1[3], box2[3])\n","        \n","        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n","        \n","        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n","        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n","        \n","        union_area = box1_area + box2_area - inter_area\n","        if union_area == 0:\n","            return 0\n","        \n","        return inter_area / union_area\n","    \n","    def combine_bboxes(box1, box2):\n","        # Combine two boxes by taking the outermost coordinates\n","        x1 = min(box1[0], box2[0])\n","        y1 = min(box1[1], box2[1])\n","        x2 = max(box1[2], box2[2])\n","        y2 = max(box1[3], box2[3])\n","        return [x1, y1, x2, y2]\n","    \n","    while len(bboxes) > max_bboxes:\n","        merged = False\n","        # Step 1: Always merge intersecting boxes first\n","        for i in range(len(bboxes)):\n","            for j in range(i + 1, len(bboxes)):\n","                if compute_iou(bboxes[i], bboxes[j]) > 0.0:  # Intersecting boxes\n","                    new_box = combine_bboxes(bboxes[i], bboxes[j])\n","                    bboxes[i] = new_box\n","                    bboxes.pop(j)\n","                    merged = True\n","                    break\n","            if merged:\n","                break\n","\n","        # Step 2: If no boxes were merged, use proximity to merge the closest pair\n","        if not merged:\n","            a = np.array(bboxes)\n","            centroids = np.array([(0.5 * (box[0] + box[2]), 0.5 * (box[1] + box[3])) for box in bboxes])\n","            pairwise_distances = np.sqrt(np.sum(np.square(centroids[:, None] - centroids[None, :]), axis=-1))\n","            \n","            # Set diagonal to infinity to avoid merging a box with itself\n","            np.fill_diagonal(pairwise_distances, np.inf)\n","            \n","            # Find the closest pair of boxes and merge them\n","            i, j = np.unravel_index(np.argmin(pairwise_distances), pairwise_distances.shape)\n","            new_box = combine_bboxes(bboxes[i], bboxes[j])\n","            bboxes[i] = new_box\n","            bboxes.pop(j)\n","\n","    return bboxes\n","\n","def draw_bboxes(img, bboxes):\n","    # Convert BGR image (OpenCV) to RGB for Matplotlib\n","    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    \n","    # Draw the final bounding boxes on the image\n","    for box in bboxes:\n","        cv2.rectangle(img_rgb, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n","    \n","    # Display the image using Matplotlib\n","    plt.figure(figsize=(3, 3))\n","    plt.imshow(img_rgb)\n","    plt.axis('off')  # Hide axes for better viewing\n","    plt.show()\n","\n","def extract_text_from_bboxes(img, bboxes):\n","    text_bbox = []\n","    for box in bboxes:\n","        # Crop the region of the image inside the bounding box\n","        cropped_img = img[box[1]:box[3], box[0]:box[2]]\n","\n","        # grayscale the cropped_img\n","        gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n","        _, threshold_image = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n","\n","        \n","        # Use pytesseract to extract text from the cropped image\n","        text = pytesseract.image_to_string(threshold_image, config='--psm 3', lang='eng')\n","        # print(text)\n","        text_bbox.append((text, box))\n","    \n","    # Clean the extracted texts\n","    clean_texts_bbox = []\n","    for text, box in text_bbox:\n","        clean_text = extract_value_from_ocr(text)\n","        # print(clean_text)\n","        if clean_text:\n","            clean_texts_bbox.append((clean_text, box))\n","    \n","    return clean_texts_bbox\n","\n","def detect_text(image_path):\n","    # Step 1: Extract bounding boxes\n","    img, bboxes = extract_bboxes(image_path)\n","    \n","    # Step 2: Merge bounding boxes\n","    merged_bboxes = merge_bboxes(bboxes, max_bboxes=10)\n","    \n","    # Step 3: Draw and display final bounding boxes in notebook\n","    # draw_bboxes(img, merged_bboxes)\n","    \n","    # Step 4: Extract text from the remaining bounding boxes\n","    texts_bboxs_final = extract_text_from_bboxes(img, merged_bboxes)\n","\n","    # Box objects\n","    bounding_boxes = []\n","    for text, box in texts_bboxs_final:\n","        x1, y1, x2, y2 = box\n","        bbox = BoundingBox(x1, y1, x2, y2, text)\n","        bounding_boxes.append(bbox)\n","    \n","    return bounding_boxes\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-15T14:22:43.982356Z","iopub.status.busy":"2024-09-15T14:22:43.982081Z","iopub.status.idle":"2024-09-15T14:24:39.614971Z","shell.execute_reply":"2024-09-15T14:24:39.614126Z","shell.execute_reply.started":"2024-09-15T14:22:43.982320Z"},"trusted":true},"outputs":[],"source":["# pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n","data = pd.read_csv(\"/kaggle/input/trainer/train.csv\")\n","\n","# Dictionary to convert units to inches\n","unit_conversion = {\n","    \"centimetre\": 0.393701,\n","    \"millimetre\": 0.0393701,\n","    \"metre\": 39.3701,\n","    \"inch\": 1,\n","    \"foot\": 12,\n","    \"yard\": 36\n","}\n","\n","# Filter rows where entity_name is height, depth, or width\n","filtered_data = data[data['entity_name'].isin(['height', 'depth', 'width'])]\n","\n","# Convert all entity values to inches\n","def convert_to_inches(value, unit):\n","    return value * unit_conversion[unit]\n","\n","# Assuming entity_value contains numeric part and a unit (e.g., '30 inch')\n","# Splitting and processing the entity_value into numeric and unit\n","filtered_data['value_in_inches'] = filtered_data['entity_value'].apply(\n","    lambda x: float(x.split()[0]) * unit_conversion[x.split()[1].lower()]\n",")\n","\n","# Grouping by group_id and entity_name and calculating mean and std deviation\n","grouped = filtered_data.groupby(['group_id', 'entity_name'])['value_in_inches']\n","\n","# Creating dictionaries for mean and standard deviation per group_id and entity_name\n","mean_dict = grouped.mean().unstack().to_dict()\n","std_dict = grouped.std().unstack().to_dict()\n","\n","# EfficientNet-based image classifier returning embeddings\n","class SingleBoxClassifier(nn.Module):\n","    def __init__(self):\n","        super(SingleBoxClassifier, self).__init__()\n","        self.efficientnet = models.efficientnet_b3(pretrained=True)\n","        self.efficientnet.classifier = nn.Identity()  # Remove the classifier layer of EfficientNet\n","        \n","        # EfficientNet-B3 output features\n","        self.feature_dim = 1536\n","    \n","    def forward(self, x):\n","        e = self.efficientnet(x)  # Extract features from EfficientNet\n","        # # Convert to NumPy matrix\n","        # e_matrix = e.detach().cpu().numpy()  # Detach from computation graph and move to CPU if needed\n","\n","        # return e_matrix  # Return a tensor\n","        e = e.detach()\n","        return e  # Return embeddings\n","    \n","\n","def generate_input_channels(image: np.array, xi: float, yi: float) -> np.array:\n","    h, w = image.shape[:2]\n","    xi, yi = xi / w, yi / h  # Normalize coordinates\n","\n","    # Convert to grayscale\n","    f1 = 0.299 * image[:,:,0] + 0.587 * image[:,:,1] + 0.114 * image[:,:,2]\n","    f1 = np.expand_dims(f1, axis=-1)  # Add channel dimension\n","\n","    # Normalize coordinates\n","    x_coords, y_coords = np.meshgrid(np.arange(w) / w, np.arange(h) / h)\n","    f2 = np.abs(x_coords - xi)\n","    f3 = np.abs(y_coords - yi)\n","    \n","    # Stack to create 3 channels\n","    return np.concatenate((f1, f2[..., np.newaxis], f3[..., np.newaxis]), axis=-1)\n","\n","def calculate_z_bar(value: float, unit: str, category: str) -> np.array:\n","    value = convert_to_inches(value, unit)\n","    return_list = []\n","    search_list = ['depth', 'width', 'height']\n","    for dim in search_list:\n","        z_alpha = (math.log(value) - mean_dict[dim][category])/std_dict[dim][category]\n","        if math.isnan(z_alpha):\n","            # print(\"true\")\n","            z_alpha = math.inf\n","        print(\"z_alpha: \",z_alpha)\n","        z_alpha_bar = math.exp(-abs(z_alpha))\n","        return_list.append(z_alpha_bar)\n","    \n","    return_list.append(1.0)\n","    print(return_list)\n","    return np.array(return_list)\n","\n","\n","\n","def process_image(image_link, category):\n","\n","    response = requests.get(image_link)\n","    img = Image.open(BytesIO(response.content))\n","\n","    image = np.array(img)\n","    boxes = detect_text(img)\n","    print(\"no of boxes:\", len(boxes))\n","    classifier = SingleBoxClassifier()\n","\n","    embeddings_list =[]\n","    text_list = []\n","    z_bar_list = []\n","\n","    for box in boxes:\n","\n","        text = box.text\n","        text_list.append(text)\n","\n","        centroid = box.get_centroid()\n","        input_channels = generate_input_channels(image, centroid[0], centroid[1])\n","        z_bar = calculate_z_bar(box.value, box.unit, category)\n","        z_bar_list.append(z_bar)\n","        \n","        # Convert input to tensor\n","        x_tensor = torch.tensor(input_channels).permute(2, 0, 1).unsqueeze(0).float()\n","        \n","        # Forward pass through the classifier\n","        with torch.no_grad():\n","            embeddings = classifier(x_tensor)\n","\n","        embeddings_list.append(embeddings)\n","\n","    return embeddings_list, text_list, z_bar_list\n","\n","\n","def processor_callback(sample):\n","    embeddings_list, text_list, z_bar_list = process_image(sample[\"image_link\"], sample[\"group_id\"])   \n","\n","    if len(embeddings_list) > 0:\n","        sample[\"embeddings_list\"] = np.array(embeddings_list, dtype = np.float32)\n","        sample[\"text_list\"] = text_list\n","        sample[\"z_bar_list\"] = np.array(z_bar_list, dtype = np.float32)\n","\n","    return sample\n","    \n","\n","if __name__ == \"__main__\":\n","    from huggingface_hub import login\n","    \n","    df = pd.read_csv(\"/kaggle/input/trainer/pivot_train.csv\")\n","#     df = df[:10]\n","    H_DF = Dataset.from_pandas(df)\n","    \n","    pinger = multiprocessing.Process(target=run_in_thread)\n","    pinger.start()\n","\n","    H_DF = H_DF.map(processor_callback, num_proc = 64)\n","\n","    pdf = H_DF.to_pandas()\n","\n","    pdf.to_csv(\"/kaggle/working/stage1_train.csv\", index = False)\n","    \n","    os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n","    login(token=\"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\")\n","\n","    H_DF.push_to_hub(\"Hemabhushan/AmazonMLChallengeStage1\")\n","    \n","    pinger.terminate()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"datasetId":5708133,"sourceId":9402787,"sourceType":"datasetVersion"}],"dockerImageVersionId":30763,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
