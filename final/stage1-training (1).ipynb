{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9402787,"sourceType":"datasetVersion","datasetId":5708133}],"dockerImageVersionId":30763,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-15T15:35:14.338582Z","iopub.execute_input":"2024-09-15T15:35:14.338988Z","iopub.status.idle":"2024-09-15T15:35:14.346040Z","shell.execute_reply.started":"2024-09-15T15:35:14.338946Z","shell.execute_reply":"2024-09-15T15:35:14.345198Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"/kaggle/input/trainer/pivot_train.csv\n/kaggle/input/trainer/train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ.pop('TPU_PROCESS_ADDRESSES')\nos.environ['XLA_USE_BF16']=\"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\nos.environ[\"PJRT_DEVICE\"] = \"TPU\"","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:35:14.347492Z","iopub.execute_input":"2024-09-15T15:35:14.347764Z","iopub.status.idle":"2024-09-15T15:35:14.361500Z","shell.execute_reply.started":"2024-09-15T15:35:14.347740Z","shell.execute_reply":"2024-09-15T15:35:14.360783Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"!rm /opt/conda/lib/libcurl.so.4 \n!ln -s /usr/lib/x86_64-linux-gnu/libcurl.so.4.8.0 /opt/conda/lib/libcurl.so.4\n!add-apt-repository ppa:alex-p/tesseract-ocr5 -y\n!apt update\n!apt install -y tesseract-ocr\n!apt install tesseract-ocr-eng\n!tesseract --version","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:35:14.362384Z","iopub.execute_input":"2024-09-15T15:35:14.362598Z","iopub.status.idle":"2024-09-15T15:35:17.973891Z","shell.execute_reply.started":"2024-09-15T15:35:14.362575Z","shell.execute_reply":"2024-09-15T15:35:17.972734Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"rm: cannot remove '/opt/conda/lib/libcurl.so.4': No such file or directory\nln: failed to create symbolic link '/opt/conda/lib/libcurl.so.4': No such file or directory\n/usr/bin/sh: 1: add-apt-repository: not found\nHit:1 http://deb.debian.org/debian bookworm InRelease\nHit:2 http://deb.debian.org/debian bookworm-updates InRelease\nHit:3 http://deb.debian.org/debian-security bookworm-security InRelease\nReading package lists... Done3m\nBuilding dependency tree... Done\nReading state information... Done\n35 packages can be upgraded. Run 'apt list --upgradable' to see them.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ntesseract-ocr is already the newest version (5.3.0-2).\n0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ntesseract-ocr-eng is already the newest version (1:4.1.0-2).\n0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\ntesseract 5.3.0\n leptonica-1.82.0\n  libgif 5.2.1 : libjpeg 6b (libjpeg-turbo 2.1.2) : libpng 1.6.39 : libtiff 4.5.0 : zlib 1.2.13 : libwebp 1.2.4 : libopenjp2 2.5.0\n Found AVX512BW\n Found AVX512F\n Found AVX2\n Found AVX\n Found FMA\n Found SSE4.1\n Found OpenMP 201511\n Found libarchive 3.6.2 zlib/1.2.13 liblzma/5.4.1 bz2lib/1.0.8 liblz4/1.9.4 libzstd/1.5.4\n Found libcurl/7.88.1 OpenSSL/3.0.13 zlib/1.2.13 brotli/1.0.9 zstd/1.5.4 libidn2/2.3.3 libpsl/0.21.2 (+libidn2/2.3.3) libssh2/1.10.0 nghttp2/1.52.0 librtmp/2.3 OpenLDAP/2.5.13\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pytesseract","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:35:17.975625Z","iopub.execute_input":"2024-09-15T15:35:17.975986Z","iopub.status.idle":"2024-09-15T15:35:21.563521Z","shell.execute_reply.started":"2024-09-15T15:35:17.975947Z","shell.execute_reply":"2024-09-15T15:35:21.562436Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/site-packages (0.3.13)\nRequirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/site-packages (from pytesseract) (10.4.0)\nRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/site-packages (from pytesseract) (24.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:35:21.566160Z","iopub.execute_input":"2024-09-15T15:35:21.566502Z","iopub.status.idle":"2024-09-15T15:35:25.317811Z","shell.execute_reply.started":"2024-09-15T15:35:21.566464Z","shell.execute_reply":"2024-09-15T15:35:25.316633Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/site-packages (3.0.0)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/site-packages (from datasets) (3.5.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets) (3.10.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from datasets) (2024.6.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets) (24.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/site-packages (from datasets) (4.66.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets) (3.15.4)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport difflib\nimport pytesseract\nfrom pytesseract import Output\nimport cv2\nfrom typing import List, Tuple, Dict\nimport matplotlib.pyplot as plt\nimport difflib\nfrom typing import List, Dict\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nimport pytesseract\nfrom pytesseract import Output\nfrom PIL import Image\nimport cv2\nimport re\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom typing import List, Tuple, Dict\nimport math\nimport torch.nn.functional as F\nimport warnings\nimport os\nimport requests\nfrom io import BytesIO\nfrom datasets import Dataset\nimport multiprocessing\nimport threading\nimport random\nimport time\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:35:25.319375Z","iopub.execute_input":"2024-09-15T15:35:25.319687Z","iopub.status.idle":"2024-09-15T15:35:25.326847Z","shell.execute_reply.started":"2024-09-15T15:35:25.319640Z","shell.execute_reply":"2024-09-15T15:35:25.326135Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def random_operations_on_tpu():\n    import torch\n    import torch_xla.core.xla_model as xm\n    import numpy as np\n\n    # Create random tensors\n    tensor_a = torch.tensor(np.random.rand(4, 4), dtype=torch.float32)\n    tensor_b = torch.tensor(np.random.rand(4, 4), dtype=torch.float32)\n\n    # Move tensors to the TPU\n    device = xm.xla_device()\n    tensor_a = tensor_a.to(device)\n    tensor_b = tensor_b.to(device)\n\n    # Randomly choose an operation: addition or multiplication\n    operation = random.choice(['add', 'multiply'])\n\n    if operation == 'add':\n        result = tensor_a + tensor_b\n    else:\n        result = tensor_a * tensor_b\n\n    # Move the result back to the CPU\n    result = result.cpu()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:35:25.327792Z","iopub.execute_input":"2024-09-15T15:35:25.328037Z","iopub.status.idle":"2024-09-15T15:35:25.342368Z","shell.execute_reply.started":"2024-09-15T15:35:25.328012Z","shell.execute_reply":"2024-09-15T15:35:25.341794Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def run_in_thread():\n    while True:\n        # Create a new thread for the operation\n        t = threading.Thread(target=random_operations_on_tpu)\n        t.start()\n\n        # Wait for the thread to finish before starting a new one\n        t.join()\n\n        # Sleep for a random time between 2 to 3 minutes\n        sleep_time = random.randint(120, 180)  # 120 seconds to 180 seconds\n        time.sleep(sleep_time)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:35:25.343167Z","iopub.execute_input":"2024-09-15T15:35:25.343406Z","iopub.status.idle":"2024-09-15T15:35:25.351038Z","shell.execute_reply.started":"2024-09-15T15:35:25.343382Z","shell.execute_reply":"2024-09-15T15:35:25.350451Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class BoundingBox:\n    def __init__(self, x1: float, y1: float, x2: float, y2: float, text: str):\n        self.x1 = x1\n        self.y1 = y1\n        self.x2 = x2\n        self.y2 = y2\n        self.text = text\n        self.value = None\n        self.unit = None\n        self.infer_text()\n        print(self.value)\n    \n    def get_centroid(self) -> Tuple[float, float]:\n        return ((self.x1 + self.x2) / 2, (self.y1 + self.y2) / 2)\n    \n    def infer_text(self):\n        value, unit = self.text.split()\n        self.value = float(value)\n        self.unit = str(unit)\n\n# Abbreviation map for units\nabbreviation_map = {\n    'in': 'inch',\n    'cm': 'centimetre',\n    'ft': 'foot',\n    'm': 'metre',\n    'mm': 'millimetre',\n    'yd': 'yard',\n    'g': 'gram',\n    'kg': 'kilogram',\n    'mg': 'milligram',\n    'ug': 'microgram',\n    'oz': 'ounce',\n    'lb': 'pound',\n    't': 'ton',\n    'v': 'volt',\n    'mv': 'millivolt',\n    'kv': 'kilovolt',\n    'w': 'watt',\n    'kw': 'kilowatt',\n    'ml': 'millilitre',\n    'l': 'litre',\n    'dl': 'decilitre',\n    'cl': 'centilitre',\n    'gal': 'gallon',\n    'pt': 'pint',\n    'qt': 'quart',\n    'cu in': 'cubic inch',\n    'cu ft': 'cubic foot',\n}\n\nentity_unit_map = {\n    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'item_weight': {'gram',\n        'kilogram',\n        'microgram',\n        'milligram',\n        'ounce',\n        'pound',\n        'ton'},\n    'maximum_weight_recommendation': {'gram',\n        'kilogram',\n        'microgram',\n        'milligram',\n        'ounce',\n        'pound',\n        'ton'},\n    'voltage': {'kilovolt', 'millivolt', 'volt'},\n    'wattage': {'kilowatt', 'watt'},\n    'item_volume': {'centilitre',\n        'cubic foot',\n        'cubic inch',\n        'cup',\n        'decilitre',\n        'fluid ounce',\n        'gallon',\n        'imperial gallon',\n        'litre',\n        'microlitre',\n        'millilitre',\n        'pint',\n        'quart'}\n}\n\n# Combine both abbreviations and full names into a single set for matching\nvalid_units = set(abbreviation_map.keys()).union(set(abbreviation_map.values()))\n\ndef correct_ocr_errors(text) -> str:\n    # print(\"Input text:\", text)\n    # Common OCR errors and their corrections for specified units\n    corrections = {\n        # Centimetre\n        \"centimetre\": \"centimetre\", \"centimeter\": \"centimetre\",\n        \"centimetres\": \"centimetre\", \"centimeters\": \"centimetre\",\n        \"centlmetre\": \"centimetre\", \"centlmetres\": \"centimetre\",\n        \"cm\": \"centimetre\", \"CM\": \"centimetre\",\n        # Foot\n        \"ft\": \"foot\", \"feet\": \"foot\", \"foots\": \"foot\",\n        # Millimetre\n        \"millimetre\": \"millimetre\", \"millimeter\": \"millimetre\",\n        \"millimetres\": \"millimetre\", \"millimeters\": \"millimetre\",\n        \"milimetre\": \"millimetre\", \"milimetres\": \"millimetre\",\n        \"mm\": \"millimetre\",\n        # Metre\n        \"metre\": \"metre\", \"meter\": \"metre\",\n        \"metres\": \"metre\", \"meters\": \"metre\",\n        \"m\": \"metre\",\n        # Inch\n        \"inch\": \"inch\", \"inches\": \"inch\",\n        \"inche\": \"inch\", \"inchs\": \"inch\",\n        \"inoh\": \"inch\", \"lnch\": \"inch\",\n        \"\\\"\": \"inch\", 'Inchy': \"inch\", \n        \"inchy\": \"inch\",\n        # Yard\n        \"yard\": \"yard\", \"yards\": \"yard\",\n        \"yrd\": \"yard\", \"yrds\": \"yard\",\n        \"yd\": \"yard\", \"yds\": \"yard\",\n    }\n\n    # Function to correct numbers\n    def correct_number(match):\n        num = match.group(0)\n        corrected = num.replace('O', '0').replace('l', '1').replace('I', '1')\n        return corrected\n\n    # Correct numbers (replace 'O' with '0', 'l' or 'I' with '1')\n    text = re.sub(r'\\d+', correct_number, text)\n\n    # Correct spacing issues (e.g., \"34. 5\" to \"34.5\")\n    text = re.sub(r'(\\d+)\\s*\\.\\s*(\\d+)', r'\\1.\\2', text)\n\n    # Split the text into words\n    words = text.split()\n\n    # Process each word\n    for i, word in enumerate(words):\n        # Check if the word has a number followed by a unit\n        match = re.match(r'(\\d+)([a-zA-Z]+)', word)\n        if match:\n            number, unit = match.groups()\n            lower_unit = unit.lower()\n            \n            # Check if the unit is in our corrections dictionary\n            if lower_unit in corrections:\n                corrected_unit = corrections[lower_unit]\n                words[i] = f\"{number} {corrected_unit}\"\n            else:\n                # Use difflib to find the closest match for the unit\n                close_matches = difflib.get_close_matches(lower_unit, corrections.keys(), n=1, cutoff=0.8)\n                if close_matches:\n                    corrected_unit = corrections[close_matches[0]]\n                    words[i] = f\"{number} {corrected_unit}\"\n        else:\n            # Process standalone words without numbers\n            lower_word = word.lower()\n            if lower_word in corrections:\n                words[i] = corrections[lower_word]\n            else:\n                close_matches = difflib.get_close_matches(lower_word, corrections.keys(), n=1, cutoff=0.8)\n                if close_matches:\n                    words[i] = corrections[close_matches[0]]\n\n    # Join the words back into a string\n    corrected_text = ' '.join(words)\n    # print(\"Output text:\", corrected_text)\n    return corrected_text\n\ndef extract_value_from_ocr(ocr_text):\n    if pd.isna(ocr_text) or not isinstance(ocr_text, str):\n        return None  # Return None if OCR text is invalid\n\n    # Pattern to capture floats/integers followed by units\n    pattern = r'(\\d+\\.?\\d*)\\s*([a-zA-Z\\s]+)'  \n    matches = re.findall(pattern, ocr_text)\n\n    # print(matches)\n\n    extracted_value = None\n    for value, unit in matches:\n        value = str(float(value))\n        unit = unit.strip().lower()  # Normalize the unit to lowercase\n        \n        # If the unit is an abbreviation, replace it with the full unit\n        if unit in abbreviation_map:\n            full_unit = abbreviation_map[unit]\n        else:\n            # If the unit is already a full unit, use it directly\n            full_unit = unit if unit in abbreviation_map.values() else correct_ocr_errors(value + ' ' + unit)\n\n        if full_unit:\n            extracted_value = value + ' ' + full_unit\n            break\n    \n    if not extracted_value or len(extracted_value.split()) != 2:\n        return None\n\n    # print(\"Extracted value:\", extracted_value, ocr_text)\n    # return extracted_value\n    final_value, final_unit = extracted_value.split()\n    if final_unit in entity_unit_map['width'] or final_unit in entity_unit_map['depth'] or final_unit in entity_unit_map['height']:\n        return extracted_value\n\n    return None\n\ndef extract_bboxes(img):\n    # Load the image\n    # img = cv2.imread(image_path)\n    \n    # Use pytesseract to get bounding boxes\n    boxes = pytesseract.image_to_data(img, output_type=Output.DICT, config='--psm 12', lang='eng')\n    \n    # Extract bounding boxes coordinates and text\n    n_boxes = len(boxes['text'])\n    bboxes = []\n    img = np.array(img)\n    img_height, img_width = img.shape[:2]  # Get image dimensions for boundary checks\n    \n    for i in range(n_boxes):\n        if boxes['text'][i].strip():\n            # Original bounding box coordinates\n            x, y, w, h = boxes['left'][i], boxes['top'][i], boxes['width'][i], boxes['height'][i]\n            \n            # Expand the bounding box by 25% in all directions\n            x_expansion = int(0.25 * w)\n            y_expansion = int(0.25 * h)\n            \n            # Calculate new coordinates\n            x_new = max(0, x - x_expansion)  # Ensure not going out of image bounds\n            y_new = max(0, y - y_expansion)\n            w_new = min(img_width, x + w + x_expansion)  # Ensure not exceeding image width\n            h_new = min(img_height, y + h + y_expansion)  # Ensure not exceeding image height\n            \n            bboxes.append([x_new, y_new, w_new, h_new])\n    \n    return img, bboxes\n\ndef merge_bboxes(bboxes, max_bboxes=3):\n    def compute_iou(box1, box2):\n        # Compute Intersection Over Union (IoU) between two boxes\n        x1 = max(box1[0], box2[0])\n        y1 = max(box1[1], box2[1])\n        x2 = min(box1[2], box2[2])\n        y2 = min(box1[3], box2[3])\n        \n        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n        \n        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n        \n        union_area = box1_area + box2_area - inter_area\n        if union_area == 0:\n            return 0\n        \n        return inter_area / union_area\n    \n    def combine_bboxes(box1, box2):\n        # Combine two boxes by taking the outermost coordinates\n        x1 = min(box1[0], box2[0])\n        y1 = min(box1[1], box2[1])\n        x2 = max(box1[2], box2[2])\n        y2 = max(box1[3], box2[3])\n        return [x1, y1, x2, y2]\n    \n    while len(bboxes) > max_bboxes:\n        merged = False\n        # Step 1: Always merge intersecting boxes first\n        for i in range(len(bboxes)):\n            for j in range(i + 1, len(bboxes)):\n                if compute_iou(bboxes[i], bboxes[j]) > 0.0:  # Intersecting boxes\n                    new_box = combine_bboxes(bboxes[i], bboxes[j])\n                    bboxes[i] = new_box\n                    bboxes.pop(j)\n                    merged = True\n                    break\n            if merged:\n                break\n\n        # Step 2: If no boxes were merged, use proximity to merge the closest pair\n        if not merged:\n            a = np.array(bboxes)\n            centroids = np.array([(0.5 * (box[0] + box[2]), 0.5 * (box[1] + box[3])) for box in bboxes])\n            pairwise_distances = np.sqrt(np.sum(np.square(centroids[:, None] - centroids[None, :]), axis=-1))\n            \n            # Set diagonal to infinity to avoid merging a box with itself\n            np.fill_diagonal(pairwise_distances, np.inf)\n            \n            # Find the closest pair of boxes and merge them\n            i, j = np.unravel_index(np.argmin(pairwise_distances), pairwise_distances.shape)\n            new_box = combine_bboxes(bboxes[i], bboxes[j])\n            bboxes[i] = new_box\n            bboxes.pop(j)\n\n    return bboxes\n\ndef draw_bboxes(img, bboxes):\n    # Convert BGR image (OpenCV) to RGB for Matplotlib\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    # Draw the final bounding boxes on the image\n    for box in bboxes:\n        cv2.rectangle(img_rgb, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n    \n    # Display the image using Matplotlib\n    plt.figure(figsize=(3, 3))\n    plt.imshow(img_rgb)\n    plt.axis('off')  # Hide axes for better viewing\n    plt.show()\n\ndef extract_text_from_bboxes(img, bboxes):\n    text_bbox = []\n    for box in bboxes:\n        # Crop the region of the image inside the bounding box\n        cropped_img = img[box[1]:box[3], box[0]:box[2]]\n\n        # grayscale the cropped_img\n        gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n        _, threshold_image = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n\n        \n        # Use pytesseract to extract text from the cropped image\n        text = pytesseract.image_to_string(threshold_image, config='--psm 3', lang='eng')\n        # print(text)\n        text_bbox.append((text, box))\n    \n    # Clean the extracted texts\n    clean_texts_bbox = []\n    for text, box in text_bbox:\n        clean_text = extract_value_from_ocr(text)\n        # print(clean_text)\n        if clean_text:\n            clean_texts_bbox.append((clean_text, box))\n    \n    return clean_texts_bbox\n\ndef detect_text(image_path):\n    # Step 1: Extract bounding boxes\n    img, bboxes = extract_bboxes(image_path)\n    \n    # Step 2: Merge bounding boxes\n    merged_bboxes = merge_bboxes(bboxes, max_bboxes=10)\n    \n    # Step 3: Draw and display final bounding boxes in notebook\n    # draw_bboxes(img, merged_bboxes)\n    \n    # Step 4: Extract text from the remaining bounding boxes\n    texts_bboxs_final = extract_text_from_bboxes(img, merged_bboxes)\n\n    # Box objects\n    bounding_boxes = []\n    for text, box in texts_bboxs_final:\n        x1, y1, x2, y2 = box\n        bbox = BoundingBox(x1, y1, x2, y2, text)\n        bounding_boxes.append(bbox)\n    \n    return bounding_boxes\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:35:25.352018Z","iopub.execute_input":"2024-09-15T15:35:25.352259Z","iopub.status.idle":"2024-09-15T15:35:25.391244Z","shell.execute_reply.started":"2024-09-15T15:35:25.352234Z","shell.execute_reply":"2024-09-15T15:35:25.390577Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\ndata = pd.read_csv(\"/kaggle/input/trainer/train.csv\")\n\n# Dictionary to convert units to inches\nunit_conversion = {\n    \"centimetre\": 0.393701,\n    \"millimetre\": 0.0393701,\n    \"metre\": 39.3701,\n    \"inch\": 1,\n    \"foot\": 12,\n    \"yard\": 36\n}\n\n# Filter rows where entity_name is height, depth, or width\nfiltered_data = data[data['entity_name'].isin(['height', 'depth', 'width'])]\n\n# Convert all entity values to inches\ndef convert_to_inches(value, unit):\n    return value * unit_conversion[unit]\n\n# Assuming entity_value contains numeric part and a unit (e.g., '30 inch')\n# Splitting and processing the entity_value into numeric and unit\nfiltered_data['value_in_inches'] = filtered_data['entity_value'].apply(\n    lambda x: float(x.split()[0]) * unit_conversion[x.split()[1].lower()]\n)\n\n# Grouping by group_id and entity_name and calculating mean and std deviation\ngrouped = filtered_data.groupby(['group_id', 'entity_name'])['value_in_inches']\n\n# Creating dictionaries for mean and standard deviation per group_id and entity_name\nmean_dict = grouped.mean().unstack().to_dict()\nstd_dict = grouped.std().unstack().to_dict()\n\n# EfficientNet-based image classifier returning embeddings\nclass SingleBoxClassifier(nn.Module):\n    def __init__(self):\n        super(SingleBoxClassifier, self).__init__()\n        self.efficientnet = models.efficientnet_b3(pretrained=True)\n        self.efficientnet.classifier = nn.Identity()  # Remove the classifier layer of EfficientNet\n        \n        # EfficientNet-B3 output features\n        self.feature_dim = 1536\n    \n    def forward(self, x):\n        e = self.efficientnet(x)  # Extract features from EfficientNet\n        # # Convert to NumPy matrix\n        # e_matrix = e.detach().cpu().numpy()  # Detach from computation graph and move to CPU if needed\n\n        # return e_matrix  # Return a tensor\n        e = e.detach().numpy()\n        \n        return e.tolist()  # Return embeddings\n    \n\ndef generate_input_channels(image: np.array, xi: float, yi: float) -> np.array:\n    h, w = image.shape[:2]\n    xi, yi = xi / w, yi / h  # Normalize coordinates\n\n    # Convert to grayscale\n    f1 = 0.299 * image[:,:,0] + 0.587 * image[:,:,1] + 0.114 * image[:,:,2]\n    f1 = np.expand_dims(f1, axis=-1)  # Add channel dimension\n\n    # Normalize coordinates\n    x_coords, y_coords = np.meshgrid(np.arange(w) / w, np.arange(h) / h)\n    f2 = np.abs(x_coords - xi)\n    f3 = np.abs(y_coords - yi)\n    \n    # Stack to create 3 channels\n    return np.concatenate((f1, f2[..., np.newaxis], f3[..., np.newaxis]), axis=-1)\n\ndef calculate_z_bar(value: float, unit: str, category: str) -> np.array:\n    value = convert_to_inches(value, unit)\n    return_list = []\n    search_list = ['depth', 'width', 'height']\n    for dim in search_list:\n        z_alpha = (math.log(value) - mean_dict[dim][category])/std_dict[dim][category]\n        if math.isnan(z_alpha):\n            # print(\"true\")\n            z_alpha = math.inf\n        print(\"z_alpha: \",z_alpha)\n        z_alpha_bar = math.exp(-abs(z_alpha))\n        return_list.append(z_alpha_bar)\n    \n    return_list.append(1.0)\n    print(return_list)\n    return return_list\n\n\n\ndef process_image(image_link, category):\n\n    response = requests.get(image_link)\n    img = Image.open(BytesIO(response.content))\n\n    image = np.array(img)\n    boxes = detect_text(img)\n    print(\"no of boxes:\", len(boxes))\n    classifier = SingleBoxClassifier()\n\n    embeddings_list =[]\n    text_list = []\n    z_bar_list = []\n\n    for box in boxes:\n\n        text = box.text\n        text_list.append(text)\n\n        centroid = box.get_centroid()\n        input_channels = generate_input_channels(image, centroid[0], centroid[1])\n        z_bar = calculate_z_bar(box.value, box.unit, category)\n        z_bar_list.append(z_bar)\n        \n        # Convert input to tensor\n        x_tensor = torch.tensor(input_channels).permute(2, 0, 1).unsqueeze(0).float()\n        \n        # Forward pass through the classifier\n        with torch.no_grad():\n            embeddings = classifier(x_tensor)\n\n        embeddings_list.append(embeddings)\n\n    return embeddings_list, text_list, z_bar_list\n\n\ndef processor_callback(batch):\n    img_urls=batch['image_link']\n    group_ids=batch['group_id']\n    embeddings_res=[]\n    text_res=[]\n    z_res=[]\n    for url,category in zip(img_urls,group_ids):\n        embeddings_list,text_list,z_list=process_image(url,category)\n        if len(embeddings_list) > 0:\n            embeddings_res.append(embeddings_list)\n            text_res.append(text_list)\n            z_res.append(z_list)\n        else:\n            embeddings_res.append(None)\n            text_res.append(None)\n            z_res.append(None)\n            \n    return {\"embeddings_list\":embeddings_res,\"text_list\":text_res,\"z_list\":z_res}\n\n\nif __name__ == \"__main__\":\n    from huggingface_hub import login\n    \n    df = pd.read_csv(\"/kaggle/input/trainer/pivot_train.csv\")\n#     df = df[:10]\n    H_DF = Dataset.from_pandas(df)\n    \n    pinger = multiprocessing.Process(target=run_in_thread)\n    pinger.start()\n\n    H_DF = H_DF.map(processor_callback, num_proc = 64, batched = True)\n\n    pdf = H_DF.to_pandas()\n\n    pdf.to_csv(\"/kaggle/working/stage1_train.csv\", index = False)\n    \n    os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n    login(token=\"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\")\n\n    H_DF.push_to_hub(\"Hemabhushan/AmazonMLChallengeStage1\")\n    \n    pinger.terminate()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T15:35:25.392186Z","iopub.execute_input":"2024-09-15T15:35:25.392465Z","iopub.status.idle":"2024-09-15T15:37:18.886276Z","shell.execute_reply.started":"2024-09-15T15:35:25.392439Z","shell.execute_reply":"2024-09-15T15:37:18.885190Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"num_proc must be <= 10. Reducing num_proc to 10 for dataset of size 10.\nMap (num_proc=10):   0%|          | 0/10 [00:00<?, ? examples/s]","output_type":"stream"},{"name":"stdout","text":"no of boxes: 0\n20.025.0\n\n45.025.0\n\nno of boxes:no of boxes:  22\n\n25.0\n25.0\nno of boxes: 2\n150.0\nno of boxes: 1\n","output_type":"stream"},{"name":"stderr","text":"Map (num_proc=10):  10%|█         | 1/10 [00:01<00:12,  1.38s/ examples]","output_type":"stream"},{"name":"stdout","text":"23.6\n15.7\nno of boxes: 2\n","output_type":"stream"},{"name":"stderr","text":"Exception in thread Thread-12 (random_operations_on_tpu):\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/usr/local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n    _threading_Thread_run(self)\n  File \"/usr/local/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/tmp/ipykernel_13/3764695990.py\", line 11, in random_operations_on_tpu\n","output_type":"stream"},{"name":"stdout","text":"300.0\n","output_type":"stream"},{"name":"stderr","text":"  File \"/usr/local/lib/python3.10/site-packages/torch_xla/core/xla_model.py\", line 212, in xla_device\n","output_type":"stream"},{"name":"stdout","text":"400.0","output_type":"stream"},{"name":"stderr","text":"    return runtime.xla_device(n, devkind)","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py\", line 95, in wrapper\n","output_type":"stream"},{"name":"stdout","text":"no of boxes: ","output_type":"stream"},{"name":"stderr","text":"    ","output_type":"stream"},{"name":"stdout","text":"2\n","output_type":"stream"},{"name":"stderr","text":"return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py\", line 124, in xla_device\n    return torch.device(torch_xla._XLAC._xla_get_default_device())\nRuntimeError: Bad StatusOr access: UNKNOWN: TPU initialization failed: open(/dev/accel0): Operation not permitted: Operation not permitted; Couldn't open device: /dev/accel0; Unable to create Node RegisterInterface for node 0, config: go/debugonly    device_path: \"/dev/accel0\" mode: KERNEL debug_data_directory: \"\" dump_anomalies_only: true crash_in_debug_dump: false allow_core_dump: true; could not create driver instance\n","output_type":"stream"},{"name":"stdout","text":"no of boxes: 0\n6.5\n8.66\n4.44\n3.14\n4.8\n4.8\nno of boxes: 6\nz_alpha:  -1.8074592425213383\nz_alpha:  -1.926072075212687\nz_alpha:  inf\n[0.16407047095278357, 0.1457194508740252, 0.0, 1.0]\nz_alpha:  -1.8074592425213383\nz_alpha:  -1.926072075212687\nz_alpha:  inf\n[0.16407047095278357, 0.1457194508740252, 0.0, 1.0]\nz_alpha: 29.5\n no of boxes:-1.8343474517715874z_alpha:   1\nz_alpha: \n -1.591556953385136-1.975330684774104\n\nz_alpha:  -1.5305438172690051z_alpha: \n infz_alpha:  inf\n\n[0.15971769126656668, 0.13871543357586824, 0.0, 1.0][0.20360835610291086, 0.2164179434937901, 0.0, 1.0]\n\nz_alpha:  -1.7854899759167921\nz_alpha:  -1.8858248606826247\nz_alpha:  inf\n[0.16771486462015509, 0.15170387324550594, 0.0, 1.0]\nz_alpha:  -1.702080151065027\nz_alpha:  -1.7330198911244097\nz_alpha:  inf\n[0.18230390969010735, 0.17674983793464905, 0.0, 1.0]\nz_alpha:  -1.9697780633373638\nz_alpha:  -2.2234365945315795\nz_alpha:  inf\n[0.13948781023909038, 0.10823650396810801, 0.0, 1.0]\n","output_type":"stream"},{"name":"stderr","text":"Map (num_proc=10):  20%|██        | 2/10 [00:09<00:43,  5.49s/ examples]","output_type":"stream"},{"name":"stdout","text":"z_alpha:  -1.7875151893344463\nz_alpha:  -1.8895350072413004\nz_alpha:  inf\n[0.16737554993428821, 0.15114207246829742, 0.0, 1.0]\n","output_type":"stream"},{"name":"stderr","text":"[globals.cc : 104] RAW: absl::log_internal::SetTimeZone() has already been called\nhttps://symbolize.stripped_domain/r/?trace=7fe7aa3d1e2c,7fe7aa38304f,56d272b7feff&map= \n*** SIGABRT received by PID 12638 (TID 26320) on cpu 82 from PID 12638; stack trace: ***\nPC: @     0x7fe7aa3d1e2c  (unknown)  (unknown)\n    @     0x7fe47ae64521        944  (unknown)\n    @     0x7fe7aa383050      18128  (unknown)\n    @     0x56d272b7ff00  (unknown)  (unknown)\nhttps://symbolize.stripped_domain/r/?trace=7fe7aa3d1e2c,7fe47ae64520,7fe7aa38304f,56d272b7feff&map= \nE0915 15:35:42.520037   26320 coredump_hook.cc:316] RAW: Remote crash data gathering hook invoked.\nE0915 15:35:42.520113   26320 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.\nE0915 15:35:42.520176   26320 coredump_hook.cc:411] RAW: Sending fingerprint to remote end.\nE0915 15:35:42.520257   26320 coredump_hook.cc:420] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory\nE0915 15:35:42.520323   26320 coredump_hook.cc:472] RAW: Dumping core locally.\nE0915 15:35:51.793601   26320 process_state.cc:805] RAW: Raising signal 6 with default behavior\n","output_type":"stream"},{"name":"stdout","text":"z_alpha:  -1.7508250451308838\nz_alpha:  -1.8223194678480026\nz_alpha:  inf\n[0.17363063123218017, 0.1616503729209494, 0.0, 1.0]\nz_alpha:  -1.8228826109785432\nz_alpha:  -1.9543273479191914\nz_alpha:  inf\n[0.16155936625118517, 0.14165973236732268, 0.0, 1.0]\nz_alpha:  -1.8074592425213383\nz_alpha:  -1.926072075212687\nz_alpha:  inf\n[0.16407047095278357, 0.1457194508740252, 0.0, 1.0]\nz_alpha:  -1.736632500061997\nz_alpha:  -1.7963190365911448\nz_alpha: z_alpha:   inf-1.8074592425213383\n\n[0.17611246187882357, 0.16590846861392225, 0.0, 1.0]\nz_alpha:  -1.926072075212687\nz_alpha:  inf\n[0.16407047095278357, 0.1457194508740252, 0.0, 1.0]\n","output_type":"stream"},{"name":"stderr","text":"Map (num_proc=10):  30%|███       | 3/10 [01:06<03:23, 29.00s/ examples]","output_type":"stream"},{"name":"stdout","text":"z_alpha:  -1.7511931887587022\nz_alpha:  -1.8229938988990426\nz_alpha:  inf\n[0.17356672198631057, 0.16154138764569723, 0.0, 1.0]\n","output_type":"stream"},{"name":"stderr","text":"Map (num_proc=10):  60%|██████    | 6/10 [01:45<01:18, 19.73s/ examples]","output_type":"stream"},{"name":"stdout","text":"z_alpha:  -1.9033821202529533\nz_alpha:  -2.1018006831766165\nz_alpha:  inf\n[0.14906361464085233, 0.12223612143367783, 0.0, 1.0]\n","output_type":"stream"},{"name":"stderr","text":"Map (num_proc=10):  80%|████████  | 8/10 [01:46<00:18,  9.48s/ examples]","output_type":"stream"},{"name":"stdout","text":"z_alpha:  -1.9451262112903587\nz_alpha:  -2.178274942312662\nz_alpha:  inf\n[0.14296917787492514, 0.1132367020944785, 0.0, 1.0]\nz_alpha:  -1.8939879601790293\nz_alpha:  -2.084590787979353\nz_alpha:  inf\n[0.150470540193133, 0.12435799854291503, 0.0, 1.0]\nz_alpha:  -1.8939879601790293\nz_alpha:  -2.084590787979353\nz_alpha:  inf\n[0.150470540193133, 0.12435799854291503, 0.0, 1.0]\n","output_type":"stream"},{"name":"stderr","text":"Map (num_proc=10): 100%|██████████| 10/10 [01:50<00:00, 11.02s/ examples]\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\nCreating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 145.23ba/s]\nUploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\nhttps://symbolize.stripped_domain/r/?trace=7fe7aa4458dc,7fe7aa38304f&map= \n*** SIGTERM received by PID 24579 (TID 24579) on cpu 47 from PID 13; stack trace: ***\nPC: @     0x7fe7aa4458dc  (unknown)  select\n    @     0x7fe58ee64521        944  (unknown)\n    @     0x7fe7aa383050       3432  (unknown)\n    @ ... and at least 1 more frames\nhttps://symbolize.stripped_domain/r/?trace=7fe7aa4458dc,7fe58ee64520,7fe7aa38304f&map= \nE0915 15:37:18.886253   24579 coredump_hook.cc:262] RAW: Remote crash gathering disabled for SIGTERM.\nE0915 15:37:18.946695   24579 process_state.cc:805] RAW: Raising signal 15 with default behavior\n","output_type":"stream"}]}]}